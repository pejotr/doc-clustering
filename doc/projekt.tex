\documentclass{article}
\usepackage[polish]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}

\begin{document}
\title{WEDT - DOKUMENTACJA WSTĘPNA}
\author{Piotr Doniec}
\date {Grudzień 2011}
\maketitle

\section{Cel projektu, zadanie}
Dany jest zbiór dokumentów tekstowych (czysty tekst oraz HTML). Należy dokonać podziału zbioru na grupy, tak aby dokumenty należące do pojedynczej grupy były jak najbardziej zbliżone tematycznie do siebie, a jednocześnie odmienne od dokumentów w pozostałych grupach. Grupy mogą (ale nie muszą) - tworzyć strukturę hierarchiczną. 

\section{Metoda rozwiazania}
\begin{enumerate}
\item Wczytanie dokumentu do analizy
\item Utworzenie w pamięci 2 kopii dokumentu: bez i z znacznikami HTML. Funkcja usuwająca HTML jest zaimplementowana w NLTK.
\item Usunięcie wszystkich znaków specjalnych, interpunkcji z przetwarzanego dokumentu przy pomocy prostego wyrażenia regularnego. 
\item Usunięcie spójników i innych słów nie mających wpływu na treść dokumentu (ang. stopwords). Biblioteka NLTK zawiera listę takich wyrazów dla kilku języków. Dla języka angielskiego twórcy biblioteki przewidzieli 127 wyrazów. 
\item Stemming tekstu, zliczenie wystepujących wyrażeń (terms). Wykorzystany jest stemmer Portera. Do prowadzenia statystyk wyrazów wykorzystany jest typ FreqDist umożliwiający łatwe uaktualnianie statystyki a także dostęp do liczby i częstotliwości wystąpienia wyrażenia bez konieczności przeprowadzania dodatkowych obliczeń. 
\item Wykorzystanie kopii zawiarającej znaczniki HTML do lepszej oceny treści. Z treści zadania wynika, że dokumenty mogą zawierać znaczniki HTML. Można wykorzystać tę własność w celu poznanania potencjalnie istotnych słów dla dokumentu. Znaczniki HTML można w łatwy sposób poszeregować względem ważności. Im ważniejszy znacznik, tym istotniejsze są słowa które zawiera. W chwili obecnej program uwzględnia tylko znacznik <TITLE></TITLE> i <H1></H1>. Wartościowanie znaczników odbywa się poprzez zwiększenie liczby wystąpień słowa o pewną stałą zadaną jako parametr.
\item Obliczenie dla każdego dokumentu współczynnika tf-idf zgodnie ze wzorem: 

\begin{math} (tf-idf)_{i,j} = tf_{i,j} * \log( \frac{|D|}{|{d: t_i \in d}|}) \end{math}

gdzie, \\*
\( tf_{i,j} \) - częstotliwość występowania wyrażenia 'i' w dokumencie 'j' \\*
\( |D| \) - liczba przetwarzanych dokumentów \\*
\( |d:t_i \in d| \) - liczba dokumentów w których występuje wyrażenie 'i'
\item Grupowanie dokumentów na podstawie współczynnika tf-idf. Aktualnie wykorzystywany jest algorytm KMeans oraz odległość Euklidesowa. Próby wykorzystania podobieństwa cosinusowego zakończyły się błędami - wysoce prawdopodobne, że to błąd w bibliotece. 
\end{enumerate}

\section{Algorytm KMeans}
Jest to jeden z najprostszych algorytmów uczących się bez wykorzystania nauczyciela (ang. unsupervised) rozwiązującym problem grupowania. Dane, punkty, a w przypadku tego projektu, dokumnety są przypisywane do grup, których liczba \emph{k} musi być z góry określona. Spośród dostępnych punktów, dokumnetów należy wybrać \emph{k} które będą centrami, rdzeniami grup. Można to zrobić w sposób losowy, ale trzeba pamiętać, że wybór punktów centralnych istotnie wpływa na wynik grupowania. Dobrze jest użyć punktów możliwie od siebie oddalonych. W kolejnym kroku, każdy dokument zostaje przypisany do grupy której rdzeń znajduje się najbliżej. Po wykonaniu tej procedury dla wszystkich dokumentów pierwszy etap działania algorytmu jest zakończony. Następuje obliczenie nowych rdzeni dla wszystkich grup poprzez odnalezienie punktu leżącego możliwie blisko środka grupy. W tym momencie ponownie trzeba przypisać dokumenty do grupy której rdzeń jest najbliżej - pętla. Algorytm kończy działanie kiedy nie obserwuje się już zmian punktów rdzeniowych.

\section{Narzędzia}
Projekt został zaimplementowany w języku Python. Wynika to z łatwości języka i ilością dostępnych bibliotek zapewniających stopień abstrakcji umożliwiający skupienie się na rozwiązaniu problemu bez koniczności implementacji algorytmów składowych.

Głównym ogniwem jest przedstawiona na wykładzie biblioteka nltk zapewniająca większość wymaganej funkcjonalności wykorzystanej w projeckie. Z poziomu nltk możliwa jest między innymi tokenizacja tekstu, dostęp do korpusów oraz stemming wyrazów. Zaimplementowane są także najpopularniejsze funkcje obliczające podobieństwo między dokumentami oraz algorytmy grupowania. Dodatkowo wykoszystano bibliotekę lxml która umożliwia w łatwe i szybkie parsowanie dokumentów HTML.

Kod projekt jest objęty systemem kontroli wersji i dostępny pod adresem: http://github.com/pejotr/doc-clustering

\section{Uruchomienie}
Kod projektu można pobrać z serwisu Github zarówno jako skompresowane archiwum ZIP, lub przy pomocy GIT CVS. Drugie rozwiązanie jest o tyle lepsze, że umożliwia szybkie uaktualnienie posiadanej wersji kodu do najnowszej dostępnej. Uruchomienie wymaga zainstalowanego interpretera python a także bibliotek \emph{nltk}, \emph{lxml2} oraz \emph{numpy}.

Dostępne parametry modyfikujące działanie algorytmu wyświetlane są po wykonaiu polecenia \emph{python main.py -{}-help} i są to:
\begin{description}
\item [\texttt{-{}-usehtml}] Używanie tagów HTML do lepszej oceny treści dokumentu, \emph{domyślnie False}
\item [\texttt{-{}-cosine}] Użycie podobieństwa cosinusowego, zamiast domyślnego euklidesowego
\item [\texttt{-{}-title}] Waga słów zawartych pomiędzy znacznikami <title></title>, \emph{domyślnie 1}
\item [\texttt{-{}-h1}] Waga słów zawartych pomiędzy znacznikami <h1></h1>, \emph{domyślnie 1}
\item [\texttt{-{}-freq}] Liczba najpopularniejszych wyrazów, wykorzystanych w procesie grupowania
\item [\texttt{-{}-groups}] Maksymalna liczba grup
\item [\texttt{-{}-repeats}] Liczba powtórzeń w algorytmie KMeans
\end{description}

\section{Testy}
Weryfikacja poprawności działania algorytmu została przeprowadzona poprzez porównanie wyniku działania programu z recznym pogrupowaniem dokumentów dokonanym przeze mnie. Zauważono, że domyślne wartości i wykorzystanie najprostszej funkcji podobieństwa ( podobieństwa Euklidesowego ) nie daje satysfakcjonujących wyników. Najlepszy rezultat uzyskano przy wykorzystananiu podobieństwa cosinusowego, wartościowowaniu znacznika <title></title> z wagą 7 przy 50 powtórzeniach. Odpowiada to uruchomieniu programu z następującymi parametrami: \emph{python2 src/main.py data/dev-data/ -{}-cosine -{}-usehtml -{}-groups=5  -{}-title=7}. Wynik działania:

\begin{table}[h]
\begin{center}
\begin{tabular}{ | c | c | }
\hline
Dokumenty & Grupa \\ \hline
42-48, 26, 30, 35 & 1 \\ \hline
5-13 & 2 \\ \hline
21-25, 27-29, 31-34, 36-41 & 0 \\ \hline
1-4, 14-16 & 4 \\ \hline
17-20 & 3 \\ \hline
\end{tabular}
\end{center}
\caption{\emph{python main.py dev-data/ -{}-cosine -{}-usehtml -{}-groups=5  -{}-title=7}}
\end{table}

Zwiększenie ilości grup do np. 7, sprawia że algorytm rozdzielił informajce dotyczące kawy do wiekszej liczby grup, jednak nie wszystkie grupy są wykorzystane. Pozostałe dokumenty nadal pozostają w tym samym klastrze.

\begin{table}[h]
\begin{center}
\begin{tabular}{ | c | c | }
\hline
Dokumenty & Grupa \\ \hline
42-48, 26, 35, 40 & 3 \\ \hline
5-13 & 4 \\ \hline
21-25, 28-29, 31, 33, 36-38 & 0 \\ \hline
27 & 2 \\ \hline
30, 32, 34, 39, 41 & 1 \\ \hline
1-4, 14-16 & 4 \\ \hline
17-20 & 3 \\ \hline
\end{tabular}
\caption{\emph{python main.py dev-data/ -{}-cosine-{}-usehtml -{}-groups=7 -{}-title=7}}
\end{center}
\end{table}

Ponieważ punkty rdzeniowe grup wybierane są losowo, wyniki przeprowadzone na innym komputerze w innym czasie mogą się nieznacznie różnic od zaprezentowanych.

\section{Wnioski}

\begin{thebibliography}{9}
\bibitem{SimMes}
A. Huang, \emph{Similarity Measures for Text Document Clustering}, Department of Computer Science, The University of Waikato, Hamilton, New Zealand
\bibitem{CompClust}
M. Steinbach, G. Karypis, V. Kumar, \emph{A Comparison of Document Clustering Techniques}, Department of Computer Science and Egineering, University of Minnesota
\bibitem{Mann}
C. Manning, H. Schütze, \emph{Foundations of Statistical Natural Language Processing},MIT Press, 1999
\end{thebibliography}
\end{document}
